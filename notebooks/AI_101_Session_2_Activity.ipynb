{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFXQGKYUc4X"
      },
      "source": [
        "2023 Â© Volintine Ander \\\\\n",
        "This notebook is for the **AI 101: Your First Step into Machine Learning with Tensorflow** workshop at Universiti Teknologi PETRONAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Classify images with CNN\n",
        "\n",
        "In this activity, you will train a neural network to label chest x-rays into two categories: \\\\\n",
        "\n",
        "* Normal (healthy)\n",
        "* Pneumonia\n",
        "\n",
        "The dataset was sourced from https://huggingface.co/datasets/trpakov/chest-xray-classification by Paul Mooney."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "Install the package ```huggingface_hub``` using pip. \\\\\n",
        "Import the libraries required:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1WtoaOHVrVh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "We will need the ```pathlib``` library to handle directories and ```hf_hub_download``` to download the dataset from Hugging FaceðŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "from huggingface_hub import hf_hub_download\n",
        "#hf_hub_download outputs a str\n",
        "path_to_data = hf_hub_download(repo_id=\"trpakov/chest-xray-classification\", local_dir = \"/root/.keras/datasets/\", filename=\"data/train.zip\", repo_type=\"dataset\")"
      ],
      "metadata": {
        "id": "Y8IQD5WsKLWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out ```path_to_data```"
      ],
      "metadata": {
        "id": "cQOnp8zR6WXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Insert code here"
      ],
      "metadata": {
        "id": "FGuPTpxH-fC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extract the zip file\n",
        "You need to import the ```zipfile``` library. Once you have imported it, run the code below."
      ],
      "metadata": {
        "id": "i74hYLE-RrXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(path_to_data, 'r') as training_archive:\n",
        "  training_archive.extractall(\"/root/.keras/datasets/data/\")"
      ],
      "metadata": {
        "id": "P0NaIlZ_y1E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the number of images in the dataset. You will need to import the libraries ```fnmatch``` and ```os```. Once imported, the code below."
      ],
      "metadata": {
        "id": "QDQEN49VVO0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dir_path = r'/root/.keras/datasets/data/PNEUMONIA'\n",
        "count_pneumonia = len(fnmatch.filter(os.listdir(dir_path), '*.jpg'))\n",
        "print('Pneumonia images:', count_pneumonia)\n",
        "\n",
        "dir_path = r'/root/.keras/datasets/data/NORMAL'\n",
        "count_normal = len(fnmatch.filter(os.listdir(dir_path), '*.jpg'))\n",
        "print('Normal images:', count_normal)\n"
      ],
      "metadata": {
        "id": "BcZ1r5NOvoDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write code that prints out the total number of images."
      ],
      "metadata": {
        "id": "0d3QFpvAVj3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Open an image from the dataset\n",
        "\n",
        "Modify the code below to find the 1st image under ```PNEUMONIA```."
      ],
      "metadata": {
        "id": "sQQl3oCYnXJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_dataset = '/root/.keras/datasets/data/'\n",
        "path_to_stuff = pathlib.Path('/root/.keras/datasets/data/').with_suffix('')\n",
        "pneumonia = list(path_to_stuff.glob('PNEUMONIA/*')) #An array of paths to each x-ray image in the PNEUMONIA folder\n",
        "PIL.Image.open(str(pneumonia[49]))"
      ],
      "metadata": {
        "id": "yppV_OJu-tm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDNn9MbIzfT"
      },
      "source": [
        "### Create a dataset\n",
        "\n",
        "Define the batch size, xray height, and xray width.\n",
        "The images will be rescaled based on the parameters. \\\\\n",
        "\n",
        "The batch size is the number of new images passed through the network in one time to update the model parameters. \\\\\n",
        "\n",
        "In one epoch the model is updated a few times. In each of those updates, the quantity of images used is equal to the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H74l2DoDI2XD"
      },
      "outputs": [],
      "source": [
        "batch_size = 30\n",
        "xray_height = 150\n",
        "xray_width = 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFBhRrrEI49z"
      },
      "source": [
        "It's good practice to use a validation split when developing your model. Use 80% of the images for training and 20% for validation. \\\\\n",
        "Train the model using a 70:30 training-validation split, and then run it again using an 80:20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIR0kRZiI_AT"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  path_to_dataset,\n",
        "  validation_split = 0.3,\n",
        "  subset = \"training\",\n",
        "  seed = 100, #Determines initial values of weights\n",
        "  image_size = (xray_height, xray_width),\n",
        "  batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  path_to_dataset,\n",
        "  validation_split = 0.3,\n",
        "  subset = \"validation\",\n",
        "  seed = 100, #Determines initial values of weights\n",
        "  image_size = (xray_height, xray_width),\n",
        "  batch_size = batch_size)"
      ],
      "metadata": {
        "id": "IPomhF7gBL--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQULyAvJC3X"
      },
      "source": [
        "You can find the class names in the `class_names` attribute on these datasets. These correspond to the directory names in alphabetical order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHAxkHX5JD3k"
      },
      "outputs": [],
      "source": [
        "categories = train_ds.class_names\n",
        "print(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoVvxSLJW9m"
      },
      "source": [
        "## View the dataset\n",
        "\n",
        "Show some of the images in the dataset with labels using ```matplotlib```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBmEA9c0JYes"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(16):\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(categories[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dr0at41KcAU"
      },
      "source": [
        "## Optimize dataset for training\n",
        "\n",
        "The following code optimizes training by loading some of the images into memory as opposed to only loading from disk as needed. This is called caching.\n",
        "\n",
        "It also loads from the dataset for a future training step while the current training step executes. This is called prefetching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOjJSm7DKoZA"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) #Caching\n",
        "valid_ds = valid_ds.cache().prefetch(buffer_size = AUTOTUNE) #Prefetching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GUnmPF4JvEf"
      },
      "source": [
        "## Normalize RGB channel values\n",
        "\n",
        "A color image contains three integers to describe the color of each pixel. They are the Red, Green, and Blue channels. Each channel integer can span from 0 to 255. Normalization rescales the integers to span from 0 to 1.\n",
        "\n",
        "Add the code below as the first line under definition of ```model```.\n",
        "\n",
        "```\n",
        "layers.Rescaling(1./255, input_shape = (xray_height, xray_width, 3))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcUTyDOPKucd"
      },
      "source": [
        "## Specify layers\n",
        "\n",
        "The model will use three convolution layers followed by a max pooling layer for each. The max pooling layer is responsible for making the model focus on larger values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6argA1K074"
      },
      "outputs": [],
      "source": [
        "num_classes = len(categories)\n",
        "\n",
        "model = Sequential([\n",
        "  \n",
        "  layers.Conv2D(16, 3, padding = 'same', activation = 'relu'), #3 input channels, 16 output channels. 3x3 convolution kernel.\n",
        "  layers.MaxPooling2D(), #Downsample 150x150 -> 75x75\n",
        "  layers.Conv2D(32, 3, padding = 'same', activation = 'relu'), #16 input channels, 32 output channels. 3x3 convolution kernel.\n",
        "  layers.MaxPooling2D(), #Downsample, 75x75 -> 37x37\n",
        "  layers.Conv2D(64, 3, padding = 'same', activation = 'relu'), #32 input channels, 64 output channels. 3x3 convolution kernel.\n",
        "  layers.MaxPooling2D(), #Downsample, 37x37 -> 18x18\n",
        "  layers.Flatten(), #1 dimensional tensor, 18*18*64 = 20736 channels/parameters\n",
        "  layers.Dense(128, activation = 'relu'), #1 dimensional tensor, 128 output channels. 128*(20736+1) = 2654336 parameters\n",
        "  layers.Dense(num_classes) #1 dimensional tensor, 2 output channel (one for each category, NORMAL and PNEUMONIA). \n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaKFzz72Lqpg"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "Use the `tf.keras.optimizers.Adam` optimizer and the `tf.keras.losses.SparseCategoricalCrossentropy` loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jloGNS1MLx3A"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = 'adam',\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMJ4DnuJL55A"
      },
      "source": [
        "### Model summary\n",
        "\n",
        "Use the `Model.summary` method to view all the layers, sizes, and associated parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llLYH-BXL7Xe"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiYHcbvaL9H-"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Run the code below to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fWToCqYMErH"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data = valid_ds,\n",
        "  epochs = epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFKdQpXMJT4"
      },
      "source": [
        "## Analyze model performance\n",
        "\n",
        "Create two plots:\n",
        "\n",
        "1. Training accuracy vs. epoch\n",
        "2. Validation accuracy vs. epoch\n",
        "\n",
        "Did the model perform well? Explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWnopEChMMCn"
      },
      "outputs": [],
      "source": [
        "accuracy = history.history['accuracy']\n",
        "valid_accuracy = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "valid_loss = history.history['val_loss']\n",
        "\n",
        "no_of_epochs = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(no_of_epochs, accuracy, label='Training Accuracy')\n",
        "plt.plot(no_of_epochs, valid_accuracy, label='Validation Accuracy')\n",
        "plt.legend(loc='lower center')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(no_of_epochs, loss, label='Training Loss')\n",
        "plt.plot(no_of_epochs, valid_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper center')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtv5VbaVb-3W"
      },
      "source": [
        "## Predict on new data\n",
        "\n",
        "Find an x-ray that was neither in the training nor validation dataset. \\\\\n",
        "See if the model can correctly distinguish between healthy and pneumonia lungs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_predict = \"Replace with the path to your uploaded image\"\n",
        "predict = tf.keras.utils.load_img(\n",
        "    path_to_predict, target_size=(xray_height, xray_width)\n",
        ")\n",
        "\n",
        "convert_to_array = tf.keras.utils.img_to_array(predict)\n",
        "convert_to_array = tf.expand_dims(convert_to_array, 0)\n",
        "\n",
        "prediction = model.predict(convert_to_array)\n",
        "chance = tf.nn.softmax(prediction[0])\n",
        "\n",
        "print(\"{:.3f}% likely to be {}\".format(100 * np.max(chance), categories[np.argmax(chance)].lower()))"
      ],
      "metadata": {
        "id": "u48MgXI_DrOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}