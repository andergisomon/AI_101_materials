{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2023 Â© Volintine Ander \\\\\n",
        "This notebook is for the **AI 101: Your First Step into Machine Learning with Tensorflow** workshop at Universiti Teknologi PETRONAS."
      ],
      "metadata": {
        "id": "DmRGk7KWnJYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "##Setup\n",
        "Welcome to Google Colab. In this activity, you will familiarize yourself with the Google Colab platform. Colab allows you to run code in the form of notebooks.\n",
        "\n",
        "Your code will not run locally, but on Google servers. This means that you will need to download and install libraries into the runtime that your code runs on."
      ],
      "metadata": {
        "id": "8Tg5C4Lgd5G4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting started\n",
        "\n",
        "Today's activities will require you to install the following packages using ```pip```:\n",
        "\n",
        "* tensorflow-text version 2.11\n",
        "* protobuf version 3.20.3\n",
        "\n",
        "Use the following command template to install both packages:\n",
        "\n",
        "```pip install [package name]==[version number]```\n",
        "\n",
        "In Colab, you need to add a code block in order to execute commands and code. \\\\\n",
        "#### **To execute terminal commands, add an exclamation mark ! before the command.**\n",
        "\\\\\n",
        "Ignore the dependency warnings, click on the restart runtime button when prompted after installing the libraries."
      ],
      "metadata": {
        "id": "rVBYRmtegbO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I'm a code block. Add your commands/code here to be executed"
      ],
      "metadata": {
        "id": "sVeFfwhclP0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import libraries\n",
        "\n",
        "There are two libraries you need to import:\n",
        "\n",
        "* ```tensorflow```\n",
        "* ```tensorflow_text```\n",
        "\n",
        "In order to import the libraries, use the code snippet below:\n",
        "```\n",
        "import [library name]\n",
        "```\n",
        "\n",
        "For ```tensorflow```, the library is conventionally nicknamed as ```tf```. You can use ```as``` to give a library a nickname.\n",
        "\n",
        "```\n",
        "import [library name] as [nickname]\n",
        "```\n",
        "Use these nicknames for the library:\n",
        "\n",
        "```tf``` for ```tensorflow``` \\\\\n",
        "```tf_text``` for ```tensorflow_text```\n"
      ],
      "metadata": {
        "id": "MEfPJo4-i8P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1\n",
        "####In this task, you will perform whitespace tokenization and Unicode Script tokenization.\n",
        "\n",
        "##Whitespace tokenization\n",
        "\n",
        "We will first access the ```WhitespaceTokenizer()``` method in ```tf_text``` by defining a variable. Name the variable whatever you want.\n",
        "\n",
        "**Example** \\\\\n",
        "```[first variable name] = tf_text.WhitespaceTokenizer()```"
      ],
      "metadata": {
        "id": "ffx33pU7mNpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenize something\n",
        "\n",
        "In order to print the tokenizer output, we first define a variable, ```output```. \\\\\n",
        "\n",
        "```output = [first variable name].tokenize([\"Insert text here\"])``` \\\\\n",
        "\n",
        "###Print the result\n",
        "\n",
        "Use ```print([variable])``` to print something in the code block."
      ],
      "metadata": {
        "id": "59ktl3J9omXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unicode Script tokenization\n",
        "\n",
        "Replace the ```WhitespaceTokenizer()``` method with ```UnicodeScriptTokenizer()``` to use the Unicode Script tokenizer. Modify the subsequent previous code to print the tokenizer output with Unicode Script tokenization."
      ],
      "metadata": {
        "id": "uCubqQw6tQTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2\n",
        "####Perform whole-word tokenization on Chinese text.\n",
        "\n",
        "Run the following code block:"
      ],
      "metadata": {
        "id": "431LSEo_uDsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"https://tfhub.dev/google/zh_segmentation/1\"\n",
        "tokenizer = tf_text.HubModuleTokenizer(model)\n",
        "output = tokenizer.tokenize([\"Insert Chinese text here\"])"
        "token = output.to_list()",
            "i = 0",
            "while i < len(output.to_list()[0]):",
            "print(token[0][i].decode("UTF-8"+"\n"))",
            "i = i+1"
      ],
      "metadata": {
        "id": "uhhMZ_g9vMuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the tokenized output as in Task 1."
      ],
      "metadata": {
        "id": "LYGIyR1fwIXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3\n",
        "####Perform subword tokenization with SentencePiece\n",
        "\n",
        "In this task, you will be using a small corpus of Kadazandusun text. You will need the ```sentencepiece``` library. Install it using pip and import it with the nickname ```sp```."
      ],
      "metadata": {
        "id": "dRzKkQJEwq1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Type the command to install the sentencepiece library here\n",
        "#Type the code to import the library with the nickname sp"
      ],
      "metadata": {
        "id": "qEpscJpztmPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import dataset and train the tokenizer model\n",
        "\n",
        "You will access the file containing 1000 lines of Kadazandusun text from the workshop's Github repo.\n",
        "\n",
        "[Click here to access the repo](https://github.com/andergisomon/AI_101_materials) \\\\\n",
        "\n",
        "Navigate to the ```dataset``` folder, click on ```tuntun_tonini.txt``` and download the file. Then upload the file into the Colab runtime.\n",
        "\n",
        "Use the code snippet below to import the dataset and train the tokenizer model. Replace ```FILEPATH``` with ```/content/[dataset filename]```. \\\\\n",
        "```\n",
        "sp.SentencePieceTrainer.Train(input=FILEPATH,\n",
        "model_prefix = 'dtp_spiece',\n",
        "vocab_size = 1600,\n",
        "pad_id = 0,\n",
        "unk_id = 1,\n",
        "bos_id = 2,\n",
        "eos_id = 3)\n",
        "```\n",
        "\\\\"
      ],
      "metadata": {
        "id": "36YGVaBIn3fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenize a Kadazandusun sentence\n",
        "\n",
        "Find a Kadazandusun sentence and tokenize it. \\\\\n",
        "Replace ```MODEL_PATH``` with ```'/content/dtp_spiece.model'```\n",
        "\n",
        "```\n",
        "model = sp.SentencePieceProcessor(model_file=str(MODEL_PATH))\n",
        "encoded = model.Encode(\"[Enter a Kadazandusun sentence]\")\n",
        "tokenized = [model.IdToPiece(id) for id in encoded]\n",
        "print(*tokenized)\n",
        "```"
      ],
      "metadata": {
        "id": "mgcbK74AtYMr"
      }
    }
  ]
}
